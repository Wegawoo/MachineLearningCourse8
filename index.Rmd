---
title: "Machine Learning Project - Predict how well 6 participants perform exercise"
author: "MA"
date: "19 October 2018"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synposis

We will assess how well 6 participants in an exercise routine perform using data from accelerometers on the belt, forearm, arm, and dumbell among other variables. We want to build a model to predict performance, using a training set to build the model, and to test our model prediction on the test set that has the outcome we are looking to assess missing i.e. the classe of the exercise performance from A to E.

The data for this project came from: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

...and is used under their permisssion as part of the Coursera Data Science Specialisation MOOC 

### Load training and test sets

```{r,echo=TRUE,cache=TRUE}
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
```

The training data is used for building the model, and testing data to test the model built on the training data.

### Remove columns which are of no use as all values are noted as being "N/A"

```{r,echo=TRUE}
testing <- testing[ , colSums(is.na(testing))==0]
```

We want to exclude unnecessary columns that could hinder the algorithm to be used for training.

### Remove first 8 columns from test set as these include data unrelated to the accelerometers, and the last column

```{r,echo=TRUE}
testing <- testing[ ,8:59]
```

These columns do not relate to data on exercise performance measured by accelerometers on the participants, so we will exclude.

### Make sure that columns in the training set are the same as the test set, but also include the outcome variable i.e. classe

```{r,echo=TRUE}
col.use <- colnames(testing)

library(dplyr)

training <- training %>% select(one_of(col.use,"classe"))
```

We want to ensure that variables used for training and to be part of the final model to test on the testing data, are in the test data as well, so both should match up.

### Confirm no NA values in training or test sets

```{r,echo=TRUE}
sum(sapply(training,function(x) sum(is.na(x))))
sum(sapply(testing,function(x) sum(is.na(x))))
```

This step is just to ensure data quality which appears to be good.

### Load caret and randomForest libaries to conduct analysis

```{r,echo=TRUE}
library(caret)
library(randomForest)
```

### Set seed to ensure reproducible results and run model with 10 fold cross validation repeated 3 times

```{r,echo=TRUE,cache=TRUE}

set.seed(1)

control <- trainControl(method="repeatedcv", number=10, repeats=3)

fit <- randomForest(formula=classe~.,data=training,replace=TRUE,trControl=control)
```

The control variable is designed to ensure that cross-validation is part of the training of the algorithm. This is to reduce variance, and so avoiding overfitting on the trainind data and being less predictive on unseen data. We split the data 10 times, and run the algorithm on each dataset, and then average the results across all 10. We do this three times, each time segmenting the training data is a different manner, so averging over 30 times in total. The reason for 10 and 3 is based on a balance between predictive power and time to run the algorithm/computational power.

The random forest algorithm is used and it performs very well in general, so on that basis I have used it here. It is good for multiclass classification, so have used it on that basis as well. Bagging (which is short for bootstrap aggregation) is used to increase the number of samples in the dataset and further reduce the variance by replicating the same values multiple times in the same bag. This is specified by "replace=TRUE". We use it to get OOB estimates.

The random forest algorithm also applies variable selection, so not all variables will be used for each tree (default is square root of passed in variables, so 7 in our case), and each tree that votes is trained based on different predictors. This makes the result more generic and less prone to overfitting.

### Assess out-of-bag error

```{r,echo=TRUE,cache=TRUE}
plot(fit,main="Out of Bag Error as a function of number of trees trained in random forest")

fit
```

This plot shows that the greater the number of trees, the lower the error on out of bag samples, which comprise of 1/3 of the data set (due to bagging). The true error rate on test data would be higher in theory, but we are not in a position to estimate this.

The fit generates the confusion matrix, showing that vast majority of cases are classified correctlty. The out of bag error is also mentioned above the confusion matrix above at 0.27%.

###Predict results on the test set
```{r,echo=TRUE,cache=TRUE}
predict(fit,newdata=testing)
```
The above are the predictions for the 20 test cases in the testing data set.